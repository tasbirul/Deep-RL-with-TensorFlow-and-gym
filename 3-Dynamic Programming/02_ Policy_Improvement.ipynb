{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement\n",
    "Implementation of Policy Improvement\n",
    "\n",
    "Parameters for Policy Improvement as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages \n",
    "import numpy as np\n",
    "from gym.envs.toy_text import frozen_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation the OpenAI Gym 'frozen_lake' environment\n",
    "frozen_lake_env = frozen_lake.FrozenLakeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement thePolicy Improvement algorithm\n",
    "\n",
    "<img src='./image/policy_improvement.jpg' width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_Improvement(object):\n",
    "    def __init__(self):\n",
    "        super(Policy_Improvement, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def action_value(self, env, V, s, gamma=1):\n",
    "        q = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q[a] += prob * (reward + gamma * V[next_state])\n",
    "        return q\n",
    "\n",
    "    def policy_improvement(self, env, V, gamma=1):\n",
    "        policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "        for s in range(env.nS):\n",
    "            q = self.action_value(env, V, s, gamma)\n",
    "            best_a = np.argwhere(q==np.max(q)).flatten()\n",
    "            policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
    "        return policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
