{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swing up a two-link robot with DQN\n",
    "<br>\n",
    "In this notebook we'll build a Deep Q-Network with TensorFlow deep learning framework and OpenAI gym <a href='https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py'>Acrobot-v1</a> environment to swing up a two-link robot. <br><br><br>\n",
    "\n",
    "# Acrobot-v1\n",
    "\n",
    "<img src='./images/Acrobot-v1.gif'>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height. <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_shape:object = None, action_size:object = None, scope:object = None) -> object:\n",
    "\n",
    "        self.max_size = 1000000\n",
    "        self.mem = []\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            self.scope = scope\n",
    "            self.input_shape = input_shape\n",
    "            self.action_size = action_size\n",
    "            self.initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "            self.states = tf.placeholder(shape=input_shape, dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(\n",
    "                self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            fc1 = fully_connected(self.states, \n",
    "                                  512, \n",
    "                                  activation_fn=tf.nn.relu,\n",
    "                                  weights_initializer=self.initializer)\n",
    "            fc2 = fully_connected(fc1, \n",
    "                                  256, \n",
    "                                  activation_fn=tf.nn.relu,\n",
    "                                  weights_initializer=self.initializer)\n",
    "            fc3 = fully_connected(fc2, \n",
    "                                  256, \n",
    "                                  activation_fn=tf.nn.relu,\n",
    "                                  weights_initializer=self.initializer)\n",
    "            self.q = fully_connected(fc3, \n",
    "                                    action_size, \n",
    "                                    activation_fn=None,\n",
    "                                    weights_initializer=self.initializer)\n",
    "            \n",
    "\n",
    "            self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.responsible_output = tf.reduce_sum(tf.multiply(\n",
    "                self.q, self.actions_onehot), axis=1, keep_dims=False)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.square(self.responsible_output - self.target_q))\n",
    "\n",
    "            self.update_model = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def action(self, sess, state):\n",
    "        q = sess.run(self.q, feed_dict={self.states: state[np.newaxis, ...]})\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def train(self, sess, batch, learning_rate, tnet):\n",
    "        assert len(batch) > 0\n",
    "        states = np.vstack(batch[:, 0])\n",
    "        actions = np.array(batch[:, 1])\n",
    "        rewards = batch[:, 2]\n",
    "        next_states = np.vstack(batch[:, 3])\n",
    "        dones = batch[:, 4]\n",
    "\n",
    "        next_q = sess.run(tnet.q, feed_dict={tnet.states: next_states})\n",
    "\n",
    "        next_q = rewards + (1. - dones.astype(np.float32)) * \\\n",
    "            learning_rate * np.amax(next_q, axis=1, keepdims=False)\n",
    "\n",
    "        sess.run(self.update_model, feed_dict={self.states: states,\n",
    "                                               self.actions: actions,\n",
    "                                               self.target_q: next_q})\n",
    "\n",
    "    def add(self, element):\n",
    "        self.mem.append(element)\n",
    "\n",
    "        if len(self.mem) > self.max_size:\n",
    "            self.mem.pop(0)\n",
    "\n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.mem))\n",
    "        return random.sample(self.mem, size)\n",
    "\n",
    "\n",
    "    def update_graph(self, from_graph:object, to_graph:object) -> object:\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_graph)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_graph)\n",
    "    \n",
    "        holder = []\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            holder.append(to_var.assign(from_var))\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce21ece07be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Pendulum-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0maction_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "target_update = 200\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001\n",
    "learning_rate = 0.99\n",
    "batch_size = 64\n",
    "EPISODES = 50\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "action_size = env.action_space.n\n",
    "input_shape = [None] + list(env.observation_space.shape)\n",
    "\n",
    "time_step = 0.0\n",
    "epsilon = epsilon_max\n",
    "\n",
    "Q_Network = DQN(input_shape=input_shape,\n",
    "                action_size=action_size, scope='Q_Network')\n",
    "Train_Network = DQN(input_shape=input_shape,\n",
    "                    action_size=action_size, scope='Train_Network')\n",
    "\n",
    "update_ops = Q_Network.update_graph('Q_Network', 'Train_Network')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "score = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPISODES):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(action_size)\n",
    "            else:\n",
    "                action = Q_Network.action(sess, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            Q_Network.add([state, action, reward, next_state, done])\n",
    "\n",
    "            time_step += 1.\n",
    "            epsilon = epsilon_min + \\\n",
    "                (epsilon_max - epsilon_min) * \\\n",
    "                np.exp(-epsilon_decay * time_step)\n",
    "\n",
    "            batch = np.array(Q_Network.sample(batch_size))\n",
    "            Q_Network.train(sess, batch, learning_rate, Train_Network)\n",
    "\n",
    "            state = np.copy(next_state)\n",
    "\n",
    "            if int(time_step) % target_update == 0:\n",
    "                sess.run(update_ops)\n",
    "                saver.save(sess, \"./checkpoints/Acrobot_DQN.ckpt\")\n",
    "\n",
    "            if done:\n",
    "                print('Epoch:', epoch, 'Total Rewards:', total_reward)\n",
    "                score.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array(score)\n",
    "totla_epoch = len(score)\n",
    "\n",
    "print(\"Total Epoch:\", totla_epoch)\n",
    "print(\"Min Reward:\", score.min())\n",
    "print(\"Max Reward:\", score.max())\n",
    "print(\"Average Reward:\", score.mean())\n",
    "print(\"Highest Performance:\", score.max(), \"Reached at:\", np.argmax(score) + 1, \"Episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(1, figsize=(18,10))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.plot(range(totla_epoch), score)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
