{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with TensorFlow & gym\n",
    "<br>\n",
    "In this notebook we'll build our very first Deep Q-Network with TensorFlow deep learning framework and OpenAI gym <a href='https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py'>CartPole-v0</a> environment. <br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_shape:object = None, action_size:object = None, scope:object = None) -> object:\n",
    "\n",
    "        if input_shape is None:\n",
    "            input_shape = [None, 4]\n",
    "        self.max_size = 1000000\n",
    "        self.mem = []\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            self.scope = scope\n",
    "            self.input_shape = input_shape\n",
    "            self.action_size = action_size\n",
    "\n",
    "            self.states = tf.placeholder(shape=input_shape, dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(\n",
    "                self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            fc1 = tf.layers.dense(self.states, 256, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 512, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 512, activation=tf.nn.relu)\n",
    "            self.q = tf.layers.dense(fc3, action_size, activation=None)\n",
    "\n",
    "            self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.responsible_output = tf.reduce_sum(tf.multiply(\n",
    "                self.q, self.actions_onehot), axis=1, keep_dims=False)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.square(self.responsible_output - self.target_q))\n",
    "\n",
    "            self.update_model = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def action(self, sess, state):\n",
    "        q = sess.run(self.q, feed_dict={self.states: state[np.newaxis, ...]})\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def train(self, sess, batch, learning_rate, tnet):\n",
    "        assert len(batch) > 0\n",
    "        states = np.vstack(batch[:, 0])\n",
    "        actions = np.array(batch[:, 1])\n",
    "        rewards = batch[:, 2]\n",
    "        next_states = np.vstack(batch[:, 3])\n",
    "        dones = batch[:, 4]\n",
    "\n",
    "        next_q = sess.run(tnet.q, feed_dict={tnet.states: next_states})\n",
    "\n",
    "        next_q = rewards + (1. - dones.astype(np.float32)) * \\\n",
    "            learning_rate * np.amax(next_q, axis=1, keepdims=False)\n",
    "\n",
    "        sess.run(self.update_model, feed_dict={self.states: states,\n",
    "                                               self.actions: actions,\n",
    "                                               self.target_q: next_q})\n",
    "\n",
    "    def add(self, element):\n",
    "        self.mem.append(element)\n",
    "\n",
    "        if len(self.mem) > self.max_size:\n",
    "            self.mem.pop(0)\n",
    "\n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.mem))\n",
    "        return random.sample(self.mem, size)\n",
    "\n",
    "\n",
    "    def update_graph(self, from_graph:object, to_graph:object) -> object:\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_graph)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_graph)\n",
    "    \n",
    "        holder = []\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            holder.append(to_var.assign(from_var))\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-beebccd25eab>:25: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch: 0 Total Rewards: 16.0\n",
      "Epoch: 1 Total Rewards: 11.0\n",
      "Epoch: 2 Total Rewards: 13.0\n",
      "Epoch: 3 Total Rewards: 16.0\n",
      "Epoch: 4 Total Rewards: 33.0\n",
      "Epoch: 5 Total Rewards: 23.0\n",
      "Epoch: 6 Total Rewards: 12.0\n",
      "Epoch: 7 Total Rewards: 15.0\n",
      "Epoch: 8 Total Rewards: 21.0\n",
      "Epoch: 9 Total Rewards: 27.0\n",
      "Epoch: 10 Total Rewards: 12.0\n",
      "Epoch: 11 Total Rewards: 25.0\n",
      "Epoch: 12 Total Rewards: 19.0\n",
      "Epoch: 13 Total Rewards: 16.0\n",
      "Epoch: 14 Total Rewards: 19.0\n",
      "Epoch: 15 Total Rewards: 28.0\n",
      "Epoch: 16 Total Rewards: 12.0\n",
      "Epoch: 17 Total Rewards: 16.0\n",
      "Epoch: 18 Total Rewards: 17.0\n",
      "Epoch: 19 Total Rewards: 15.0\n",
      "Epoch: 20 Total Rewards: 27.0\n",
      "Epoch: 21 Total Rewards: 12.0\n",
      "Epoch: 22 Total Rewards: 23.0\n",
      "Epoch: 23 Total Rewards: 17.0\n",
      "Epoch: 24 Total Rewards: 18.0\n",
      "Epoch: 25 Total Rewards: 31.0\n",
      "Epoch: 26 Total Rewards: 32.0\n",
      "Epoch: 27 Total Rewards: 16.0\n",
      "Epoch: 28 Total Rewards: 68.0\n",
      "Epoch: 29 Total Rewards: 27.0\n",
      "Epoch: 30 Total Rewards: 8.0\n",
      "Epoch: 31 Total Rewards: 25.0\n",
      "Epoch: 32 Total Rewards: 28.0\n",
      "Epoch: 33 Total Rewards: 27.0\n",
      "Epoch: 34 Total Rewards: 27.0\n",
      "Epoch: 35 Total Rewards: 46.0\n",
      "Epoch: 36 Total Rewards: 37.0\n",
      "Epoch: 37 Total Rewards: 52.0\n",
      "Epoch: 38 Total Rewards: 156.0\n",
      "Epoch: 39 Total Rewards: 91.0\n",
      "Epoch: 40 Total Rewards: 73.0\n",
      "Epoch: 41 Total Rewards: 90.0\n",
      "Epoch: 42 Total Rewards: 116.0\n",
      "Epoch: 43 Total Rewards: 78.0\n",
      "Epoch: 44 Total Rewards: 118.0\n",
      "Epoch: 45 Total Rewards: 137.0\n",
      "Epoch: 46 Total Rewards: 173.0\n",
      "Epoch: 47 Total Rewards: 200.0\n",
      "Epoch: 48 Total Rewards: 182.0\n",
      "Epoch: 49 Total Rewards: 200.0\n"
     ]
    }
   ],
   "source": [
    "target_update = 200\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001\n",
    "learning_rate = 0.99\n",
    "batch_size = 64\n",
    "EPISODES = 50\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "action_size = env.action_space.n\n",
    "input_shape = [None] + list(env.observation_space.shape)\n",
    "\n",
    "time_step = 0.0\n",
    "epsilon = epsilon_max\n",
    "\n",
    "Q_Network = DQN(input_shape=input_shape,\n",
    "                action_size=action_size, scope='Q_Network')\n",
    "Train_Network = DQN(input_shape=input_shape,\n",
    "                    action_size=action_size, scope='Train_Network')\n",
    "\n",
    "update_ops = Q_Network.update_graph('Q_Network', 'Train_Network')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPISODES):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(action_size)\n",
    "            else:\n",
    "                action = Q_Network.action(sess, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            Q_Network.add([state, action, reward, next_state, done])\n",
    "\n",
    "            time_step += 1.\n",
    "            epsilon = epsilon_min + \\\n",
    "                (epsilon_max - epsilon_min) * \\\n",
    "                np.exp(-epsilon_decay * time_step)\n",
    "\n",
    "            batch = np.array(Q_Network.sample(batch_size))\n",
    "            Q_Network.train(sess, batch, learning_rate, Train_Network)\n",
    "\n",
    "            state = np.copy(next_state)\n",
    "\n",
    "            if int(time_step) % target_update == 0:\n",
    "                sess.run(update_ops)\n",
    "                saver.save(sess, \"./checkpoints/CartPole_DQN.ckpt\")\n",
    "\n",
    "            if done:\n",
    "                print('Epoch:', epoch, 'Total Rewards:', total_reward)\n",
    "                break\n",
    "                \n",
    "    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
