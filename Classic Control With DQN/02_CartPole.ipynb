{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with TensorFlow & gym\n",
    "<br>\n",
    "In this notebook we'll build our very first Deep Q-Network with TensorFlow deep learning framework and OpenAI gym <a href='https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py'>CartPole-v0</a> environment. <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, input_shape:object = None, action_size:object = None, scope:object = None) -> object:\n",
    "\n",
    "        if input_shape is None:\n",
    "            input_shape = [None, 4]\n",
    "        self.max_size = 1000000\n",
    "        self.mem = []\n",
    "\n",
    "        with tf.variable_scope(scope):\n",
    "            self.scope = scope\n",
    "            self.input_shape = input_shape\n",
    "            self.action_size = action_size\n",
    "\n",
    "            self.states = tf.placeholder(shape=input_shape, dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(\n",
    "                self.actions, action_size, dtype=tf.float32)\n",
    "\n",
    "            fc1 = tf.layers.dense(self.states, 256, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            self.q = tf.layers.dense(fc2, action_size, activation=None)\n",
    "\n",
    "            self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.responsible_output = tf.reduce_sum(tf.multiply(\n",
    "                self.q, self.actions_onehot), axis=1, keep_dims=False)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.square(self.responsible_output - self.target_q))\n",
    "\n",
    "            self.update_model = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def action(self, sess, state):\n",
    "        q = sess.run(self.q, feed_dict={self.states: state[np.newaxis, ...]})\n",
    "        return np.argmax(q)\n",
    "\n",
    "    def train(self, sess, batch, learning_rate, tnet):\n",
    "        assert len(batch) > 0\n",
    "        states = np.vstack(batch[:, 0])\n",
    "        actions = np.array(batch[:, 1])\n",
    "        rewards = batch[:, 2]\n",
    "        next_states = np.vstack(batch[:, 3])\n",
    "        dones = batch[:, 4]\n",
    "\n",
    "        next_q = sess.run(tnet.q, feed_dict={tnet.states: next_states})\n",
    "\n",
    "        next_q = rewards + (1. - dones.astype(np.float32)) * \\\n",
    "            learning_rate * np.amax(next_q, axis=1, keepdims=False)\n",
    "\n",
    "        sess.run(self.update_model, feed_dict={self.states: states,\n",
    "                                               self.actions: actions,\n",
    "                                               self.target_q: next_q})\n",
    "\n",
    "    def add(self, element):\n",
    "        self.mem.append(element)\n",
    "\n",
    "        if len(self.mem) > self.max_size:\n",
    "            self.mem.pop(0)\n",
    "\n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.mem))\n",
    "        return random.sample(self.mem, size)\n",
    "\n",
    "\n",
    "    def update_graph(self, from_graph:object, to_graph:object) -> object:\n",
    "        from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_graph)\n",
    "        to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_graph)\n",
    "    \n",
    "        holder = []\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            holder.append(to_var.assign(from_var))\n",
    "        return holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-beebccd25eab>:25: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch: 0 Total Rewards: 19.0\n",
      "Epoch: 1 Total Rewards: 20.0\n",
      "Epoch: 2 Total Rewards: 17.0\n",
      "Epoch: 3 Total Rewards: 13.0\n",
      "Epoch: 4 Total Rewards: 13.0\n",
      "Epoch: 5 Total Rewards: 20.0\n",
      "Epoch: 6 Total Rewards: 9.0\n",
      "Epoch: 7 Total Rewards: 18.0\n",
      "Epoch: 8 Total Rewards: 31.0\n",
      "Epoch: 9 Total Rewards: 32.0\n",
      "Epoch: 10 Total Rewards: 14.0\n",
      "Epoch: 11 Total Rewards: 20.0\n",
      "Epoch: 12 Total Rewards: 17.0\n",
      "Epoch: 13 Total Rewards: 30.0\n",
      "Epoch: 14 Total Rewards: 12.0\n",
      "Epoch: 15 Total Rewards: 19.0\n",
      "Epoch: 16 Total Rewards: 10.0\n",
      "Epoch: 17 Total Rewards: 13.0\n",
      "Epoch: 18 Total Rewards: 26.0\n",
      "Epoch: 19 Total Rewards: 26.0\n",
      "Epoch: 20 Total Rewards: 12.0\n",
      "Epoch: 21 Total Rewards: 61.0\n",
      "Epoch: 22 Total Rewards: 21.0\n",
      "Epoch: 23 Total Rewards: 42.0\n",
      "Epoch: 24 Total Rewards: 13.0\n",
      "Epoch: 25 Total Rewards: 13.0\n",
      "Epoch: 26 Total Rewards: 21.0\n",
      "Epoch: 27 Total Rewards: 14.0\n",
      "Epoch: 28 Total Rewards: 70.0\n",
      "Epoch: 29 Total Rewards: 37.0\n",
      "Epoch: 30 Total Rewards: 21.0\n",
      "Epoch: 31 Total Rewards: 24.0\n",
      "Epoch: 32 Total Rewards: 44.0\n",
      "Epoch: 33 Total Rewards: 17.0\n",
      "Epoch: 34 Total Rewards: 25.0\n",
      "Epoch: 35 Total Rewards: 33.0\n",
      "Epoch: 36 Total Rewards: 44.0\n",
      "Epoch: 37 Total Rewards: 22.0\n",
      "Epoch: 38 Total Rewards: 40.0\n",
      "Epoch: 39 Total Rewards: 57.0\n",
      "Epoch: 40 Total Rewards: 93.0\n",
      "Epoch: 41 Total Rewards: 109.0\n",
      "Epoch: 42 Total Rewards: 200.0\n",
      "Epoch: 43 Total Rewards: 29.0\n",
      "Epoch: 44 Total Rewards: 165.0\n",
      "Epoch: 45 Total Rewards: 154.0\n",
      "Epoch: 46 Total Rewards: 161.0\n",
      "Epoch: 47 Total Rewards: 200.0\n",
      "Epoch: 48 Total Rewards: 191.0\n",
      "Epoch: 49 Total Rewards: 190.0\n",
      "Epoch: 50 Total Rewards: 200.0\n",
      "Epoch: 51 Total Rewards: 190.0\n",
      "Epoch: 52 Total Rewards: 200.0\n",
      "Epoch: 53 Total Rewards: 200.0\n",
      "Epoch: 54 Total Rewards: 200.0\n",
      "Epoch: 55 Total Rewards: 200.0\n",
      "Epoch: 56 Total Rewards: 156.0\n",
      "Epoch: 57 Total Rewards: 200.0\n",
      "Epoch: 58 Total Rewards: 200.0\n",
      "Epoch: 59 Total Rewards: 111.0\n",
      "Epoch: 60 Total Rewards: 200.0\n",
      "Epoch: 61 Total Rewards: 114.0\n",
      "Epoch: 62 Total Rewards: 200.0\n",
      "Epoch: 63 Total Rewards: 138.0\n",
      "Epoch: 64 Total Rewards: 105.0\n",
      "Epoch: 65 Total Rewards: 107.0\n",
      "Epoch: 66 Total Rewards: 200.0\n",
      "Epoch: 67 Total Rewards: 145.0\n",
      "Epoch: 68 Total Rewards: 148.0\n",
      "Epoch: 69 Total Rewards: 129.0\n",
      "Epoch: 70 Total Rewards: 126.0\n",
      "Epoch: 71 Total Rewards: 124.0\n",
      "Epoch: 72 Total Rewards: 144.0\n",
      "Epoch: 73 Total Rewards: 124.0\n",
      "Epoch: 74 Total Rewards: 147.0\n",
      "Epoch: 75 Total Rewards: 119.0\n",
      "Epoch: 76 Total Rewards: 121.0\n",
      "Epoch: 77 Total Rewards: 108.0\n",
      "Epoch: 78 Total Rewards: 117.0\n",
      "Epoch: 79 Total Rewards: 131.0\n",
      "Epoch: 80 Total Rewards: 160.0\n",
      "Epoch: 81 Total Rewards: 110.0\n",
      "Epoch: 82 Total Rewards: 131.0\n",
      "Epoch: 83 Total Rewards: 119.0\n",
      "Epoch: 84 Total Rewards: 112.0\n",
      "Epoch: 85 Total Rewards: 106.0\n",
      "Epoch: 86 Total Rewards: 103.0\n",
      "Epoch: 87 Total Rewards: 94.0\n",
      "Epoch: 88 Total Rewards: 111.0\n",
      "Epoch: 89 Total Rewards: 121.0\n",
      "Epoch: 90 Total Rewards: 164.0\n",
      "Epoch: 91 Total Rewards: 108.0\n",
      "Epoch: 92 Total Rewards: 106.0\n",
      "Epoch: 93 Total Rewards: 131.0\n",
      "Epoch: 94 Total Rewards: 121.0\n",
      "Epoch: 95 Total Rewards: 117.0\n",
      "Epoch: 96 Total Rewards: 114.0\n",
      "Epoch: 97 Total Rewards: 107.0\n",
      "Epoch: 98 Total Rewards: 109.0\n",
      "Epoch: 99 Total Rewards: 106.0\n",
      "Epoch: 100 Total Rewards: 100.0\n",
      "Epoch: 101 Total Rewards: 113.0\n",
      "Epoch: 102 Total Rewards: 103.0\n",
      "Epoch: 103 Total Rewards: 116.0\n",
      "Epoch: 104 Total Rewards: 142.0\n",
      "Epoch: 105 Total Rewards: 113.0\n",
      "Epoch: 106 Total Rewards: 114.0\n",
      "Epoch: 107 Total Rewards: 128.0\n",
      "Epoch: 108 Total Rewards: 122.0\n",
      "Epoch: 109 Total Rewards: 109.0\n",
      "Epoch: 110 Total Rewards: 110.0\n",
      "Epoch: 111 Total Rewards: 116.0\n",
      "Epoch: 112 Total Rewards: 170.0\n",
      "Epoch: 113 Total Rewards: 131.0\n",
      "Epoch: 114 Total Rewards: 119.0\n",
      "Epoch: 115 Total Rewards: 139.0\n",
      "Epoch: 116 Total Rewards: 111.0\n",
      "Epoch: 117 Total Rewards: 113.0\n",
      "Epoch: 118 Total Rewards: 173.0\n",
      "Epoch: 119 Total Rewards: 107.0\n",
      "Epoch: 120 Total Rewards: 106.0\n",
      "Epoch: 121 Total Rewards: 98.0\n",
      "Epoch: 122 Total Rewards: 105.0\n",
      "Epoch: 123 Total Rewards: 118.0\n",
      "Epoch: 124 Total Rewards: 115.0\n",
      "Epoch: 125 Total Rewards: 124.0\n",
      "Epoch: 126 Total Rewards: 108.0\n",
      "Epoch: 127 Total Rewards: 122.0\n",
      "Epoch: 128 Total Rewards: 107.0\n",
      "Epoch: 129 Total Rewards: 100.0\n",
      "Epoch: 130 Total Rewards: 108.0\n",
      "Epoch: 131 Total Rewards: 104.0\n",
      "Epoch: 132 Total Rewards: 153.0\n",
      "Epoch: 133 Total Rewards: 121.0\n",
      "Epoch: 134 Total Rewards: 140.0\n",
      "Epoch: 135 Total Rewards: 93.0\n",
      "Epoch: 136 Total Rewards: 105.0\n",
      "Epoch: 137 Total Rewards: 140.0\n",
      "Epoch: 138 Total Rewards: 100.0\n",
      "Epoch: 139 Total Rewards: 107.0\n",
      "Epoch: 140 Total Rewards: 126.0\n",
      "Epoch: 141 Total Rewards: 112.0\n",
      "Epoch: 142 Total Rewards: 200.0\n",
      "Epoch: 143 Total Rewards: 200.0\n",
      "Epoch: 144 Total Rewards: 115.0\n",
      "Epoch: 145 Total Rewards: 102.0\n",
      "Epoch: 146 Total Rewards: 119.0\n",
      "Epoch: 147 Total Rewards: 110.0\n",
      "Epoch: 148 Total Rewards: 109.0\n",
      "Epoch: 149 Total Rewards: 105.0\n",
      "Epoch: 150 Total Rewards: 156.0\n",
      "Epoch: 151 Total Rewards: 119.0\n",
      "Epoch: 152 Total Rewards: 123.0\n",
      "Epoch: 153 Total Rewards: 109.0\n",
      "Epoch: 154 Total Rewards: 104.0\n",
      "Epoch: 155 Total Rewards: 124.0\n",
      "Epoch: 156 Total Rewards: 129.0\n",
      "Epoch: 157 Total Rewards: 111.0\n",
      "Epoch: 158 Total Rewards: 103.0\n",
      "Epoch: 159 Total Rewards: 113.0\n",
      "Epoch: 160 Total Rewards: 153.0\n",
      "Epoch: 161 Total Rewards: 200.0\n",
      "Epoch: 162 Total Rewards: 36.0\n",
      "Epoch: 163 Total Rewards: 123.0\n",
      "Epoch: 164 Total Rewards: 134.0\n",
      "Epoch: 165 Total Rewards: 104.0\n",
      "Epoch: 166 Total Rewards: 144.0\n",
      "Epoch: 167 Total Rewards: 110.0\n",
      "Epoch: 168 Total Rewards: 132.0\n",
      "Epoch: 169 Total Rewards: 97.0\n",
      "Epoch: 170 Total Rewards: 100.0\n",
      "Epoch: 171 Total Rewards: 106.0\n",
      "Epoch: 172 Total Rewards: 100.0\n",
      "Epoch: 173 Total Rewards: 200.0\n",
      "Epoch: 174 Total Rewards: 129.0\n",
      "Epoch: 175 Total Rewards: 116.0\n",
      "Epoch: 176 Total Rewards: 133.0\n",
      "Epoch: 177 Total Rewards: 200.0\n",
      "Epoch: 178 Total Rewards: 108.0\n",
      "Epoch: 179 Total Rewards: 100.0\n",
      "Epoch: 180 Total Rewards: 105.0\n",
      "Epoch: 181 Total Rewards: 119.0\n",
      "Epoch: 182 Total Rewards: 146.0\n",
      "Epoch: 183 Total Rewards: 200.0\n",
      "Epoch: 184 Total Rewards: 200.0\n",
      "Epoch: 185 Total Rewards: 200.0\n",
      "Epoch: 186 Total Rewards: 200.0\n",
      "Epoch: 187 Total Rewards: 200.0\n",
      "Epoch: 188 Total Rewards: 142.0\n",
      "Epoch: 189 Total Rewards: 145.0\n",
      "Epoch: 190 Total Rewards: 111.0\n",
      "Epoch: 191 Total Rewards: 200.0\n",
      "Epoch: 192 Total Rewards: 200.0\n",
      "Epoch: 193 Total Rewards: 200.0\n",
      "Epoch: 194 Total Rewards: 200.0\n",
      "Epoch: 195 Total Rewards: 200.0\n",
      "Epoch: 196 Total Rewards: 105.0\n",
      "Epoch: 197 Total Rewards: 200.0\n",
      "Epoch: 198 Total Rewards: 200.0\n",
      "Epoch: 199 Total Rewards: 200.0\n",
      "Epoch: 200 Total Rewards: 200.0\n",
      "Epoch: 201 Total Rewards: 200.0\n",
      "Epoch: 202 Total Rewards: 200.0\n",
      "Epoch: 203 Total Rewards: 200.0\n",
      "Epoch: 204 Total Rewards: 200.0\n",
      "Epoch: 205 Total Rewards: 200.0\n",
      "Epoch: 206 Total Rewards: 200.0\n",
      "Epoch: 207 Total Rewards: 126.0\n",
      "Epoch: 208 Total Rewards: 138.0\n",
      "Epoch: 209 Total Rewards: 118.0\n",
      "Epoch: 210 Total Rewards: 200.0\n",
      "Epoch: 211 Total Rewards: 200.0\n",
      "Epoch: 212 Total Rewards: 200.0\n",
      "Epoch: 213 Total Rewards: 200.0\n",
      "Epoch: 214 Total Rewards: 200.0\n",
      "Epoch: 215 Total Rewards: 200.0\n",
      "Epoch: 216 Total Rewards: 200.0\n",
      "Epoch: 217 Total Rewards: 200.0\n",
      "Epoch: 218 Total Rewards: 200.0\n",
      "Epoch: 219 Total Rewards: 200.0\n",
      "Epoch: 220 Total Rewards: 200.0\n",
      "Epoch: 221 Total Rewards: 200.0\n",
      "Epoch: 222 Total Rewards: 200.0\n",
      "Epoch: 223 Total Rewards: 200.0\n",
      "Epoch: 224 Total Rewards: 200.0\n",
      "Epoch: 225 Total Rewards: 200.0\n",
      "Epoch: 226 Total Rewards: 200.0\n",
      "Epoch: 227 Total Rewards: 200.0\n",
      "Epoch: 228 Total Rewards: 200.0\n",
      "Epoch: 229 Total Rewards: 196.0\n",
      "Epoch: 230 Total Rewards: 200.0\n",
      "Epoch: 231 Total Rewards: 146.0\n",
      "Epoch: 232 Total Rewards: 200.0\n",
      "Epoch: 233 Total Rewards: 200.0\n",
      "Epoch: 234 Total Rewards: 200.0\n",
      "Epoch: 235 Total Rewards: 200.0\n",
      "Epoch: 236 Total Rewards: 200.0\n",
      "Epoch: 237 Total Rewards: 200.0\n",
      "Epoch: 238 Total Rewards: 200.0\n",
      "Epoch: 239 Total Rewards: 200.0\n",
      "Epoch: 240 Total Rewards: 169.0\n",
      "Epoch: 241 Total Rewards: 171.0\n",
      "Epoch: 242 Total Rewards: 199.0\n",
      "Epoch: 243 Total Rewards: 148.0\n",
      "Epoch: 244 Total Rewards: 173.0\n",
      "Epoch: 245 Total Rewards: 200.0\n",
      "Epoch: 246 Total Rewards: 200.0\n",
      "Epoch: 247 Total Rewards: 200.0\n",
      "Epoch: 248 Total Rewards: 200.0\n",
      "Epoch: 249 Total Rewards: 200.0\n",
      "Epoch: 250 Total Rewards: 200.0\n",
      "Epoch: 251 Total Rewards: 200.0\n",
      "Epoch: 252 Total Rewards: 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 253 Total Rewards: 200.0\n",
      "Epoch: 254 Total Rewards: 200.0\n",
      "Epoch: 255 Total Rewards: 200.0\n",
      "Epoch: 256 Total Rewards: 200.0\n",
      "Epoch: 257 Total Rewards: 200.0\n",
      "Epoch: 258 Total Rewards: 200.0\n",
      "Epoch: 259 Total Rewards: 200.0\n",
      "Epoch: 260 Total Rewards: 200.0\n",
      "Epoch: 261 Total Rewards: 200.0\n",
      "Epoch: 262 Total Rewards: 200.0\n",
      "Epoch: 263 Total Rewards: 200.0\n",
      "Epoch: 264 Total Rewards: 200.0\n",
      "Epoch: 265 Total Rewards: 200.0\n",
      "Epoch: 266 Total Rewards: 199.0\n",
      "Epoch: 267 Total Rewards: 119.0\n",
      "Epoch: 268 Total Rewards: 200.0\n",
      "Epoch: 269 Total Rewards: 200.0\n",
      "Epoch: 270 Total Rewards: 184.0\n",
      "Epoch: 271 Total Rewards: 163.0\n",
      "Epoch: 272 Total Rewards: 117.0\n",
      "Epoch: 273 Total Rewards: 200.0\n",
      "Epoch: 274 Total Rewards: 157.0\n",
      "Epoch: 275 Total Rewards: 200.0\n",
      "Epoch: 276 Total Rewards: 200.0\n",
      "Epoch: 277 Total Rewards: 200.0\n",
      "Epoch: 278 Total Rewards: 200.0\n",
      "Epoch: 279 Total Rewards: 200.0\n",
      "Epoch: 280 Total Rewards: 200.0\n",
      "Epoch: 281 Total Rewards: 200.0\n",
      "Epoch: 282 Total Rewards: 200.0\n",
      "Epoch: 283 Total Rewards: 200.0\n",
      "Epoch: 284 Total Rewards: 200.0\n",
      "Epoch: 285 Total Rewards: 200.0\n",
      "Epoch: 286 Total Rewards: 200.0\n",
      "Epoch: 287 Total Rewards: 200.0\n",
      "Epoch: 288 Total Rewards: 200.0\n",
      "Epoch: 289 Total Rewards: 200.0\n",
      "Epoch: 290 Total Rewards: 200.0\n",
      "Epoch: 291 Total Rewards: 200.0\n",
      "Epoch: 292 Total Rewards: 200.0\n",
      "Epoch: 293 Total Rewards: 200.0\n",
      "Epoch: 294 Total Rewards: 200.0\n",
      "Epoch: 295 Total Rewards: 200.0\n",
      "Epoch: 296 Total Rewards: 200.0\n",
      "Epoch: 297 Total Rewards: 200.0\n",
      "Epoch: 298 Total Rewards: 200.0\n",
      "Epoch: 299 Total Rewards: 200.0\n"
     ]
    }
   ],
   "source": [
    "target_update = 200\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.001\n",
    "learning_rate = 0.99\n",
    "batch_size = 64\n",
    "EPISODES = 300\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "action_size = env.action_space.n\n",
    "input_shape = [None] + list(env.observation_space.shape)\n",
    "\n",
    "time_step = 0.0\n",
    "epsilon = epsilon_max\n",
    "\n",
    "Q_Network = DQN(input_shape=input_shape,\n",
    "                action_size=action_size, scope='Q_Network')\n",
    "Train_Network = DQN(input_shape=input_shape,\n",
    "                    action_size=action_size, scope='Train_Network')\n",
    "\n",
    "update_ops = Q_Network.update_graph('Q_Network', 'Train_Network')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPISODES):\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(action_size)\n",
    "            else:\n",
    "                action = Q_Network.action(sess, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            Q_Network.add([state, action, reward, next_state, done])\n",
    "\n",
    "            time_step += 1.\n",
    "            epsilon = epsilon_min + \\\n",
    "                (epsilon_max - epsilon_min) * \\\n",
    "                np.exp(-epsilon_decay * time_step)\n",
    "\n",
    "            batch = np.array(Q_Network.sample(batch_size))\n",
    "            Q_Network.train(sess, batch, learning_rate, Train_Network)\n",
    "\n",
    "            state = np.copy(next_state)\n",
    "\n",
    "            if int(time_step) % target_update == 0:\n",
    "                sess.run(update_ops)\n",
    "                saver.save(sess, \"./checkpoints/CartPole_DQN.ckpt\")\n",
    "\n",
    "            if done:\n",
    "                print('Epoch:', epoch, 'Total Rewards:', total_reward)\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
